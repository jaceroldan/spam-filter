{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b53fc686",
   "metadata": {},
   "source": [
    "# Spam Filter\n",
    "\n",
    "This is an implementation of a Spam Filter using a Naïve Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db3eb5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b323675",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_vocabulary = []\n",
    "ham_line_paths = []\n",
    "spam_line_paths = []\n",
    "with open('./labels', 'r') as file:\n",
    "    for line in file.readlines():\n",
    "        if re.match(r'ham', line):\n",
    "            ham_line_paths.append(line)\n",
    "        elif re.match(r'spam', line):\n",
    "            spam_line_paths.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf08464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_word_vectors(filename, line_paths, classification, master_vocabulary):\n",
    "    full_vocabulary = {}\n",
    "    full_file_vocabs = []\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "        for line_path in line_paths:\n",
    "            path = os.path.join(os.getcwd(), re.sub(r'{} \\.\\.\\/'.format(classification), '', line_path)).strip()\n",
    "            with open(path, 'rb') as file:\n",
    "                file_vocab = {'[[file_path]]': line_path.strip()}\n",
    "\n",
    "                for line in file.readlines():\n",
    "                    try:\n",
    "                        split_words = re.findall(r'[A-Za-z]+', line.decode('utf-8'))\n",
    "                    except UnicodeDecodeError:\n",
    "                        continue\n",
    "                    for w in split_words:\n",
    "                        if full_vocabulary.get(w.lower(), None):\n",
    "                            full_vocabulary[w.lower()] += 1\n",
    "                        else:\n",
    "                            full_vocabulary[w.lower()] = 1\n",
    "\n",
    "                        if file_vocab.get(w.lower(), None):\n",
    "                            file_vocab[w.lower()] += 1\n",
    "                        else:\n",
    "                            file_vocab[w.lower()] = 1\n",
    "                        master_vocabulary.append(w)\n",
    "                full_file_vocabs.append(file_vocab)\n",
    "\n",
    "#         full_vocabulary_wordset = sorted(full_vocabulary.keys())\n",
    "#         rows_to_add = [['DOCUMENT', *full_vocabulary_wordset]]\n",
    "        print('vocabulary sorted!')\n",
    "#         for file_path in line_paths:\n",
    "#             row = [file_path]\n",
    "#             print(file_path)\n",
    "#             vocab_dict = next((vocab for vocab in full_file_vocabs if vocab['[[file_path]]'] == file_path), None)\n",
    "#             if vocab_dict:\n",
    "#                 row.extend([vocab_dict.get(word, 0) for word in full_vocabulary_set])\n",
    "#             else:\n",
    "#                 row.extend([0] * len(full_vocabulary))\n",
    "#             rows_to_add.append(row)\n",
    "#         print('rows loaded!')\n",
    "#         writer.writerows(rows_to_add)\n",
    "    \n",
    "    return full_vocabulary, full_file_vocabs, list(set(master_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aae4cb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary sorted!\n",
      "2130014\n"
     ]
    }
   ],
   "source": [
    "ham_vocabulary, ham_file_vocabs, master_vocabulary = write_word_vectors('hamvectors.csv', ham_line_paths, 'ham', master_vocabulary)\n",
    "print(len(ham_vocabulary.values()))\n",
    "ham_full = [*ham_file_vocabs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37bbfaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary sorted!\n",
      "260921\n"
     ]
    }
   ],
   "source": [
    "spam_vocabulary, spam_file_vocabs, master_vocabulary = write_word_vectors('spamvectors.csv', spam_line_paths, 'spam', master_vocabulary)\n",
    "print(len(spam_vocabulary.values()))\n",
    "spam_full = [*spam_file_vocabs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1b94310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_split(dataset, training_percentage):\n",
    "    dataset_length = len(dataset)\n",
    "    sample_count = (dataset_length * (training_percentage / 100)) // 1\n",
    "    training_sample = []\n",
    "\n",
    "    while sample_count > 0:\n",
    "        current_index = random.randint(0, len(dataset) - 1)\n",
    "        training_sample.append(dataset[current_index])\n",
    "        del dataset[current_index]\n",
    "        sample_count -= 1\n",
    "\n",
    "    return training_sample, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b6c235e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12910\n",
      "9037 3873\n",
      "24912\n",
      "17438 7474\n"
     ]
    }
   ],
   "source": [
    "# Split into 70% training and 30% test set\n",
    "ham_training, ham_test = take_split(ham_full, 70)\n",
    "print(len(ham_file_vocabs))\n",
    "print(len(ham_training), len(ham_test))\n",
    "\n",
    "spam_training, spam_test = take_split(spam_full, 70)\n",
    "print(len(spam_file_vocabs))\n",
    "print(len(spam_training), len(spam_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10e92e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior probabilities for Spam and Ham: \n",
      "ham class:  9037 / 26475\n",
      "spam class:  17438  /  26475\n"
     ]
    }
   ],
   "source": [
    "print('prior probabilities for Spam and Ham: ')\n",
    "print('ham class: ', len(ham_training), '/', len(ham_training) + len(spam_training))\n",
    "print('spam class: ', len(spam_training),  ' / ', len(ham_training) + len(spam_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "018c8673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only care about how many times this word occurred across different document Ds\n",
    "rebuilt_ham_vocabulary_probabilities = {}\n",
    "for item in ham_training:\n",
    "    for word in item:\n",
    "        if rebuilt_ham_vocabulary_probabilities.get(word, None):\n",
    "            rebuilt_ham_vocabulary_probabilities[word] += 1\n",
    "        else:\n",
    "            rebuilt_ham_vocabulary_probabilities[word] = 1\n",
    "\n",
    "for key in rebuilt_ham_vocabulary_probabilities.keys():\n",
    "    rebuilt_ham_vocabulary_probabilities[key] = (rebuilt_ham_vocabulary_probabilities[key] + 1) / (len(ham_training) + len(master_vocabulary))\n",
    "    \n",
    "ham_vocabulary_size = len(rebuilt_ham_vocabulary_probabilities.keys())\n",
    "\n",
    "rebuilt_spam_vocabulary_probabilities = {}\n",
    "for item in spam_training:\n",
    "    for word in item:\n",
    "        if rebuilt_spam_vocabulary_probabilities.get(word, None):\n",
    "            rebuilt_spam_vocabulary_probabilities[word] += 1\n",
    "        else:\n",
    "            rebuilt_spam_vocabulary_probabilities[word] = 1\n",
    "\n",
    "for key in rebuilt_spam_vocabulary_probabilities:\n",
    "    rebuilt_spam_vocabulary_probabilities[key] = (rebuilt_spam_vocabulary_probabilities[key] + 1) / (len(spam_training) + len(master_vocabulary))\n",
    "\n",
    "spam_vocabulary_size = len(rebuilt_spam_vocabulary_probabilities.keys())\n",
    "\n",
    "full_vocabulary_size = len(set(list(rebuilt_ham_vocabulary_probabilities.keys()) + list(rebuilt_spam_vocabulary_probabilities.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a767439",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_to_test(test_set, ham_training, spam_training, lamb):\n",
    "    classified_as_ham = 0\n",
    "    classified_as_spam = 0\n",
    "    for item in test_set:\n",
    "        ham_probability = len(ham_training) / (len(ham_training) + len(ham_test))\n",
    "        spam_probability = len(spam_training) / (len(spam_training) + len(spam_test))\n",
    "\n",
    "        ham_default = 1 / (len(ham_training) + lamb * len(ham_training))\n",
    "        spam_default = 1 / (len(spam_training) + lamb * len(spam_training))\n",
    "        for word in item.keys():\n",
    "            ham_component = rebuilt_ham_vocabulary_probabilities.get(word)\n",
    "            spam_component = rebuilt_spam_vocabulary_probabilities.get(word)\n",
    "            ham_percent = ham_component if ham_component else ham_default\n",
    "            spam_percent = spam_component if spam_component else spam_default\n",
    "\n",
    "            if ham_probability * ham_percent == 0:\n",
    "                break\n",
    "            if spam_probability * spam_percent == 0:\n",
    "                break\n",
    "            ham_probability *= ham_percent\n",
    "            spam_probability *= spam_percent\n",
    "\n",
    "        if ham_probability > spam_probability:\n",
    "            classified_as_ham += 1\n",
    "        else:\n",
    "            classified_as_spam += 1\n",
    "\n",
    "    return classified_as_ham, classified_as_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60399e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified ham (TN):  1701 / 7474\n",
      "Classified spam (FP):  2172 / 7474\n",
      "Classified ham (FN):  181 / 7474\n",
      "Classified spam (TP):  7293 / 7474\n"
     ]
    }
   ],
   "source": [
    "classified_as_ham = 0\n",
    "classified_as_spam = 0\n",
    "\n",
    "classified_as_ham, classified_as_spam = apply_to_test(ham_test, ham_training, spam_training, 1)\n",
    "print('Classified ham (TN): ', classified_as_ham, '/', len(spam_test))\n",
    "tn = classified_as_ham\n",
    "print('Classified spam (FP): ', classified_as_spam, '/', len(spam_test))\n",
    "fp = classified_as_spam\n",
    "\n",
    "classified_as_ham, classified_as_spam = apply_to_test(spam_test, ham_training, spam_training, 1)\n",
    "print('Classified ham (FN): ', classified_as_ham, '/', len(spam_test))\n",
    "fn = classified_as_ham\n",
    "print('Classified spam (TP): ', classified_as_spam, '/', len(spam_test))\n",
    "tp = classified_as_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45ea3cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.7705229793977812\n",
      "recall:  0.9757827134064758\n"
     ]
    }
   ],
   "source": [
    "print('precision: ', tp / (tp + fp))\n",
    "print('recall: ', tp / (tp + fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c763b9",
   "metadata": {},
   "source": [
    "# Lambda Smoothing tests\n",
    "\n",
    "We will now be applying different values of lambda into different runs of our classifier to measure differences in precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33128ad",
   "metadata": {},
   "source": [
    "## At λ = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a738d03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified ham (TN):  1701 / 7474\n",
      "Classified spam (FP):  2172 / 7474\n",
      "Classified ham (FN):  181 / 7474\n",
      "Classified spam (TP):  7293 / 7474\n",
      "precision:  0.7705229793977812\n",
      "recall:  0.9757827134064758\n"
     ]
    }
   ],
   "source": [
    "classified_as_ham = 0\n",
    "classified_as_spam = 0\n",
    "\n",
    "classified_as_ham, classified_as_spam = apply_to_test(ham_test, ham_training, spam_training, lamb=1)\n",
    "print('Classified ham (TN): ', classified_as_ham, '/', len(spam_test))\n",
    "tn = classified_as_ham\n",
    "print('Classified spam (FP): ', classified_as_spam, '/', len(spam_test))\n",
    "fp = classified_as_spam\n",
    "\n",
    "classified_as_ham, classified_as_spam = apply_to_test(spam_test, ham_training, spam_training, lamb=1)\n",
    "print('Classified ham (FN): ', classified_as_ham, '/', len(spam_test))\n",
    "fn = classified_as_ham\n",
    "print('Classified spam (TP): ', classified_as_spam, '/', len(spam_test))\n",
    "tp = classified_as_spam\n",
    "\n",
    "print('precision: ', tp / (tp + fp))\n",
    "print('recall: ', tp / (tp + fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5163b65d",
   "metadata": {},
   "source": [
    "## At λ = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28a84afe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified ham (TN):  1881 / 7474\n",
      "Classified spam (FP):  1992 / 7474\n",
      "Classified ham (FN):  114 / 7474\n",
      "Classified spam (TP):  7360 / 7474\n",
      "precision:  0.7869974337040205\n",
      "recall:  0.9847471233609848\n"
     ]
    }
   ],
   "source": [
    "classified_as_ham = 0\n",
    "classified_as_spam = 0\n",
    "\n",
    "classified_as_ham, classified_as_spam = apply_to_test(ham_test, ham_training, spam_training, lamb=2)\n",
    "print('Classified ham (TN): ', classified_as_ham, '/', len(spam_test))\n",
    "tn = classified_as_ham\n",
    "print('Classified spam (FP): ', classified_as_spam, '/', len(spam_test))\n",
    "fp = classified_as_spam\n",
    "\n",
    "classified_as_ham, classified_as_spam = apply_to_test(spam_test, ham_training, spam_training, lamb=2)\n",
    "print('Classified ham (FN): ', classified_as_ham, '/', len(spam_test))\n",
    "fn = classified_as_ham\n",
    "print('Classified spam (TP): ', classified_as_spam, '/', len(spam_test))\n",
    "tp = classified_as_spam\n",
    "\n",
    "print('precision: ', tp / (tp + fp))\n",
    "print('recall: ', tp / (tp + fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e139e15",
   "metadata": {},
   "source": [
    "## At λ = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7cccc8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified ham (TN):  1599 / 7474\n",
      "Classified spam (FP):  2274 / 7474\n",
      "Classified ham (FN):  235 / 7474\n",
      "Classified spam (TP):  7239 / 7474\n",
      "precision:  0.760958688111006\n",
      "recall:  0.9685576665774686\n"
     ]
    }
   ],
   "source": [
    "classified_as_ham = 0\n",
    "classified_as_spam = 0\n",
    "\n",
    "classified_as_ham, classified_as_spam = apply_to_test(ham_test, ham_training, spam_training, lamb=0.5)\n",
    "print('Classified ham (TN): ', classified_as_ham, '/', len(spam_test))\n",
    "tn = classified_as_ham\n",
    "print('Classified spam (FP): ', classified_as_spam, '/', len(spam_test))\n",
    "fp = classified_as_spam\n",
    "\n",
    "classified_as_ham, classified_as_spam = apply_to_test(spam_test, ham_training, spam_training, lamb=0.5)\n",
    "print('Classified ham (FN): ', classified_as_ham, '/', len(spam_test))\n",
    "fn = classified_as_ham\n",
    "print('Classified spam (TP): ', classified_as_spam, '/', len(spam_test))\n",
    "tp = classified_as_spam\n",
    "\n",
    "print('precision: ', tp / (tp + fp))\n",
    "print('recall: ', tp / (tp + fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cad258e",
   "metadata": {},
   "source": [
    "## At λ = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3b43f2b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified ham (TN):  1478 / 7474\n",
      "Classified spam (FP):  2395 / 7474\n",
      "Classified ham (FN):  292 / 7474\n",
      "Classified spam (TP):  7182 / 7474\n",
      "precision:  0.7499216873760051\n",
      "recall:  0.960931228257961\n"
     ]
    }
   ],
   "source": [
    "classified_as_ham = 0\n",
    "classified_as_spam = 0\n",
    "\n",
    "classified_as_ham, classified_as_spam = apply_to_test(ham_test, ham_training, spam_training, lamb=0.1)\n",
    "print('Classified ham (TN): ', classified_as_ham, '/', len(spam_test))\n",
    "tn = classified_as_ham\n",
    "print('Classified spam (FP): ', classified_as_spam, '/', len(spam_test))\n",
    "fp = classified_as_spam\n",
    "\n",
    "classified_as_ham, classified_as_spam = apply_to_test(spam_test, ham_training, spam_training, lamb=0.1)\n",
    "print('Classified ham (FN): ', classified_as_ham, '/', len(spam_test))\n",
    "fn = classified_as_ham\n",
    "print('Classified spam (TP): ', classified_as_spam, '/', len(spam_test))\n",
    "tp = classified_as_spam\n",
    "\n",
    "print('precision: ', tp / (tp + fp))\n",
    "print('recall: ', tp / (tp + fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c5c0b8",
   "metadata": {},
   "source": [
    "## At λ = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055199f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_as_ham = 0\n",
    "classified_as_spam = 0\n",
    "\n",
    "classified_as_ham, classified_as_spam = apply_to_test(ham_test, ham_training, spam_training, lamb=0.005)\n",
    "print('Classified ham (TN): ', classified_as_ham, '/', len(spam_test))\n",
    "tn = classified_as_ham\n",
    "print('Classified spam (FP): ', classified_as_spam, '/', len(spam_test))\n",
    "fp = classified_as_spam\n",
    "\n",
    "classified_as_ham, classified_as_spam = apply_to_test(spam_test, ham_training, spam_training, lamb=0.005)\n",
    "print('Classified ham (FN): ', classified_as_ham, '/', len(spam_test))\n",
    "fn = classified_as_ham\n",
    "print('Classified spam (TP): ', classified_as_spam, '/', len(spam_test))\n",
    "tp = classified_as_spam\n",
    "\n",
    "print('precision: ', tp / (tp + fp))\n",
    "print('recall: ', tp / (tp + fn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
